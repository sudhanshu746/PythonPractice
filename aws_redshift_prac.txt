

AKIAIZKBLOBV63BCWC4A

7FTQPvTL4qx8E2ILR5s/Q4iHHxSn8Jzz9H0E+C7o


jdbc:redshift://examplecluster.ca8hlitale3p.us-east-2.redshift.amazonaws.com:5439/dev?user=admin&password=Ashu!1234

Driver={Amazon Redshift (x64)}; Server=examplecluster.ca8hlitale3p.us-east-2.redshift.amazonaws.com; Database=dev; UID=admin; PWD=insert_your_master_user_password_here; Port=5439
Ashu!123

examplecluster.ca8hlitale3p.us-east-2.redshift.amazonaws.com:5439;dev



CREATE TABLE test 
(
 Year                   VARCHAR(20),
Month                  VARCHAR(20),
DayofMonth             VARCHAR(20),
DayOfWeek              VARCHAR(20),
DepTime                VARCHAR(20),
CRSDepTime             VARCHAR(20),
ArrTime                VARCHAR(20),
CRSArrTime             VARCHAR(20),
UniqueCarrier          VARCHAR(20),
FlightNum              VARCHAR(20),
TailNum                VARCHAR(20),
ActualElapsedTime      VARCHAR(20),
CRSElapsedTime         VARCHAR(20),
AirTime                VARCHAR(20),
ArrDelay               VARCHAR(20),
DepDelay               VARCHAR(20),
Origin                 VARCHAR(20),
Dest                   VARCHAR(20),
Distance               VARCHAR(20),
TaxiIn                 VARCHAR(20),
TaxiOut                VARCHAR(20),
Cancelled              VARCHAR(20),
CancellationCode       VARCHAR(20),
Diverted               VARCHAR(20),
CarrierDelay           VARCHAR(20),
WeatherDelay           VARCHAR(20),
NASDelay               VARCHAR(20),
SecurityDelay          VARCHAR(20),
LateAircraftDelay      VARCHAR(20)
);

copy test from
's3://examplecluster123/flights'
credentials 'aws_access_key_id=AKIAIZKBLOBV63BCWC4A;aws_secret_access_key=7FTQPvTL4qx8E2ILR5s/Q4iHHxSn8Jzz9H0E+C7o'
csv;



jdbc:postgresql://examplecluster.ca8hlitale3p.us-east-2.redshift.amazonaws.com:5439/dev?user=admin&password=Ashu!123

examplecluster.ca8hlitale3p.us-east-2.redshift.amazonaws.com:



/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic postgres_users



---- Databricks
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SparkSession

 val session = SparkSession.builder()
    .master("local")
    .appName("POC")
    .getOrCreate()

session.conf.set("fs.s3n.awsAccessKeyId", "AKIAIZKBLOBV63BCWC4A")
session.conf.set("fs.s3n.awsSecretAccessKey", "7FTQPvTL4qx8E2ILR5s/Q4iHHxSn8Jzz9H0E+C7o")

val eventsDF = session.read
    .format("com.databricks.spark.redshift").option("url","jdbc:redshift://examplecluster.ca8hlitale3p.us-east-2.redshift.amazonaws.com:5439/dev?user=admin&password=Ashu!123").option("query", "select count(*) from test").option("tempdir", "s3n://examplecluster123/tmp") .option("aws_iam_role", "arn:aws:iam::352784915116:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift").load()

