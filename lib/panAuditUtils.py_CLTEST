#===============================================================================
#
#         FILE:  panAuditUtils.py
#
#  DESCRIPTION:  Python module for logging auditing information to an HBase table.
#
#                Contains class panAudit
#
#                USAGE:
#
#                 from pyspark import SparkContext, SparkConf
#                 import panAuditUtils
#                 import sys
#                 import traceback
#
#                 def mapFunc (val):
#                     return val*10
#
#                 conf = SparkConf().setAppName("Spark Hello")
#                 sc = SparkContext(conf=conf)
#
#                 # Script level audit message.  From spark make sure to pass the Application ID
#                 scriptAud = panAuditUtils.panAudit( "Main", sc.applicationId)
#                 try:
#
#                     # Audit message for a unit of work.  Notice that this references the audit ID of the script
#                     # level audit message.
#                     rddAud = panAuditUtils.panAudit( "Start of RDD Work", sc.applicationId, scriptAud.get_audit_id())
#                     theRDD = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9]).map(mapFunc)
#
#                     rddResults = theRDD.collect()
#
#                     # Log that we successfully completed the unit of work.
#                     rddAud.update("SUCCESS", "Processed %d records" % len(rddResults))
#
#                     # Deliberately throw an exception
#                     myVar = 17 / 0
#
#                     # This is where a normal successful completion gets logged.
#                     scriptAud.update("SUCCESS")
#
#                 except:
#
#                     # Log the exception to the database
#                     excpMsg = traceback.format_exception(sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2])
#                     scriptAud.update("ERROR", excpMsg)
#
#
#===============================================================================
#  Modifications
#===============================================================================
#
# Date         : 9/26/2017
# Modified By  : Casey Catalano
# Modification : Original Version
#===============================================================================

import panConfig
import uuid
import happybase
import datetime
import inspect

"""
Utility to log audit progress to the audit_history table in HBase.


"""



class panAudit:

    def __init__(self, desc, app_id = 'NonSpark', parent_audit_id = None, envVal = None, hbaseThriftServers = None, audit_id = None):

        """
        Populate all values and write an initial record to hbase table audit_history.

        The status and status_msg can be updated with the update call.

        Supply envVal and hbaseThriftServers if this will not be called from the Edge node as the panera.ini
        file is only available on the Edge Node.
        """

        # If needed the the value of environment and a comma seperated list of HBase Thrift servers string from panConfig
        theConfig = panConfig.panConfig()
        if envVal:
            self.envVal = envVal
        else:
            self.envVal = theConfig.get('envVal')
        if hbaseThriftServers:
            self.hbaseThriftServers = hbaseThriftServers
        else:
            self.hbaseThriftServers = theConfig.get('hbaseThriftServers').split(',')

        if not audit_id:
   
           # Assign this audit record a unique ID.  Log the parent audit_id.
           self.audit_id = str(uuid.uuid4())
           self.parent_audit_id = parent_audit_id
   
           # Set the start and end timestamps to the current time
           tsNow = datetime.datetime.now()
           self.start_dttm = str(tsNow)
           self.end_dttm = str(tsNow)
   
           # Use the inspect module to get the name of the current script and line number of the caller from
           # the call stack.
           self.script_name = inspect.stack()[1][1]
           self.script_line = str(inspect.stack()[1][2])
   
           # Initial status will always be INPROG.  A status_msg has to be supplied by a call to update.
           self.status = 'INPROG'
           self.status_msg = None
   
           # Description of what we are logging.
           self.desc = desc
   
           # A spark job will have an APP ID.  Yarn aggregated logs can be helpful in troubleshooting.
           self.app_id = app_id
   
         
           self.writeHbase()

        else:

           conn = self.connect()

           # Do an hbase get and set self... to what comes back from Hbase 
           tableName = self.envVal + ":audit_history"
           table = conn.table(tableName)
           row = table.row (audit_id)
           self.audit_id = audit_id
           self.start_dttm = row['a:sd']
           self.end_dttm = row['a:ed']
           self.script_name = row['a:sn']
           self.script_line = row['a:sl']
           self.status = row['a:s']

           try:
               self.parent_audit_id = row['a:pai']
           except:
               self.parent_audit_id = None
           if parent_audit_id:
               self.parent_audit_id = parent_audit_id
              
           try:
               self.status_msg =  row['a:sm']
           except:
               self.status_msg =  None
           self.desc =  row['a:d']
           self.app_id = row['a:ai']


    def update (self, status, status_msg = None):    

        """
        Update the status and status_msg values and write them to hbase table audit_history
        """
        if status not in ['INPROG', 'SUCCESS', 'WARN', 'ERROR']:
            raise ValueError ("Invalid status.  Valid values are INPROG, SUCCESS, WARN, ERROR")
        self.status = status

        # status_msg has to be a string to be written to HBase.  You can pass an exception if desired.
        self.status_msg = str(status_msg)

        # Update end_dttm timestamp.
        tsNow = datetime.datetime.now()
        self.end_dttm = str(tsNow)

        # Update the line number of the caller from the call stack.
        self.script_line = str(inspect.stack()[1][2])
        self.writeHbase()
 

    def __repr__(self):
        return str(self.__dict__)


    def connect(self):
        # Setup a happybase connection to an hbase thrift server.  Try all of the
        # servers before giving up.
        conn = None
        #thriftIter = iter(self.hbaseThriftServers)
        conn = happybase.Connection(host='10.71.239.14', port=9090, use_kerberos=True)
        #conn = happybase.Connection('10.71.239.14', use_kerberos=True)
        #while not isinstance(conn, happybase.connection.Connection):
        #   hbaseThriftServer = thriftIter.next()
        #   try:
        #       conn = happybase.Connection(hbaseThriftServer, port=9090,  use_kerberos=True)
        #   except:
        #      pass

        return conn
        
    def writeHbase(self):

        """
        Write to hbase table audit_history
        """

        try:
            # Setup a happybase connection to an hbase thrift server.  Try all of the
            # servers before giving up.
            conn = self.connect()

            # Write to the audit_history table.
            tableName = self.envVal + ":audit_history"
            table = conn.table(tableName)
            rowDict = {'a:sd' : self.start_dttm,
                       'a:ed' : self.end_dttm,
                       'a:sn' : self.script_name,
                       'a:sl' : self.script_line,
                       'a:s' : self.status,
                       'a:d' : self.desc,
                       'a:ai' : self.app_id }
            if self.status_msg:
                rowDict['a:sm'] = self.status_msg
            if self.parent_audit_id:
                rowDict['a:pai'] = self.parent_audit_id
            table.put (self.audit_id, rowDict)
        except:
            # Just write a message to standard out.  We don't want to throw an exception
            # on a logging failure.
            print "Error writing Audit Log %s" % self.__repr__()
            raise 
       
       
    def get_audit_id(self):
        return self.audit_id
